{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed105df-2ac1-46f8-b48d-860af4fb3954",
   "metadata": {},
   "source": [
    "## Create SparkSession and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb14e8a8-a5a3-497a-8836-f809b2dceac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/23 21:28:32 WARN Utils: Your hostname, krxps resolves to a loopback address: 127.0.1.1; using 192.168.68.61 instead (on interface wlp0s20f3)\n",
      "24/01/23 21:28:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/23 21:28:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Window\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdba2d-228e-4e2f-8342-f1482e4729f5",
   "metadata": {},
   "source": [
    "## Create a dataframe (with SparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f34fb72-5c62-4475-8417-c22505e391a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,), (2,)], \"id: int\")\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"test\")\n",
    "\n",
    "df_table = spark.table(\"test\")\n",
    "df_table.show()\n",
    "\n",
    "df_sql = spark.sql(\"select id from test where id = 2\")\n",
    "df_sql.show()\n",
    "\n",
    "df_range = spark.range(0, 3)\n",
    "df_range.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98975c23-ce58-4f90-a8c2-299bee652e36",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa08828d-6b90-4479-8878-de5e1416cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\"), (None, None)], [\"id\", \"name\"])\n",
    "\n",
    "df.filter((col(\"id\") == 1) | (col(\"name\") == \"a\")).take(5)\n",
    "\n",
    "df.filter(\"id == 1 or name == 'a'\").take(5)\n",
    "\n",
    "assert df.filter(col(\"id\").isNull()).count() == 1\n",
    "assert df.filter(isnull(col(\"id\"))).count() == 1\n",
    "\n",
    "assert df.filter(col(\"id\").isNotNull()).count() == 2\n",
    "assert df.filter(~isnull(\"id\")).count() == 2\n",
    "assert df.filter(\"id is not null\").count() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433384e-992a-48ac-a15b-9e3e00e60b69",
   "metadata": {},
   "source": [
    "## Access column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3392712-9a37-4a81-baf0-b7dcfb5f5432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'id[type]'>\n",
      "Column<'id[type]'>\n",
      "Column<'id'>\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,), (2,)], \"id long\")\n",
    "\n",
    "print(df.id)\n",
    "print(df[\"id\"])\n",
    "print(col(\"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656e9732-cd52-45a5-9e68-fcfebd27eb1e",
   "metadata": {},
   "source": [
    "## Create column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48bd7e71-f595-4c66-acdc-e1a74302827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|squared|\n",
      "+---+-------+\n",
      "|  1|    1.0|\n",
      "|  2|    4.0|\n",
      "+---+-------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,), (2,)], \"id long\")\n",
    "\n",
    "df.withColumn(\"squared\", pow(\"id\", 2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2605af-0022-4249-83fd-4bdb70f0ea2e",
   "metadata": {},
   "source": [
    "## Rename a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72cdfda2-3059-456f-abfd-2a05338342fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---------+\n",
      "|id_test|name|name_test|\n",
      "+-------+----+---------+\n",
      "|      1|test|     test|\n",
      "|      2|  yo|       yo|\n",
      "+-------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"test\"), (2, \"yo\")], \"id long, name string\")\n",
    "\n",
    "(df.withColumnRenamed(\"id\", \"id_test\")\n",
    " .withColumn(\"name_test\", col(\"name\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1749e9e2-50cb-4507-8a6c-2ff54de54c44",
   "metadata": {},
   "source": [
    "## Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ffa429f-bd6c-4639-9d02-ca5a145ab1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+\n",
      "| id|squared|function|\n",
      "+---+-------+--------+\n",
      "|  0|    0.0|  square|\n",
      "|  1|    1.0|  square|\n",
      "|  2|    4.0|  square|\n",
      "+---+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(0, 3)\n",
    "\n",
    "(df\n",
    " .withColumn(\"squared\", pow(\"id\", lit(2)))\n",
    " .withColumn(\"function\", lit(\"square\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01746bda-1ad4-4da5-9000-64ffb9c44d91",
   "metadata": {},
   "source": [
    "## Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c60c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=10, name='a')\n",
      "+-------+\n",
      "|boolean|\n",
      "+-------+\n",
      "|  false|\n",
      "|   true|\n",
      "|   true|\n",
      "|   true|\n",
      "|  false|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(10, \"a\"), (20, \"b\"), (30, \"c\"), (40, \"d\"), (40, \"e\")], [\"id\", \"name\"])\n",
    "\n",
    "print(df.select(\"id\", \"name\").first())\n",
    "df.select((col(\"id\").between(20, 40) & col(\"name\").isin(\"b\", \"c\", \"d\")).alias(\"boolean\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da20b34-c3e2-4960-a1b8-259e20d8a03a",
   "metadata": {},
   "source": [
    "## Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "592dbe87-4067-4e3d-a308-c12f642254fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+--------------+------+----+--------+\n",
      "| _1|test| array|second_element|sorted|size|contains|\n",
      "+---+----+------+--------------+------+----+--------+\n",
      "|  1| a_b|[a, b]|             b|[b, a]|   2|   false|\n",
      "+---+----+------+--------------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,)])\n",
    "\n",
    "(df\n",
    " .withColumn(\"test\", lit(\"a_b\"))\n",
    " .withColumn(\"array\", split(col(\"test\"), \"_\"))\n",
    " .withColumn(\"second_element\", col(\"array\")[1])\n",
    " .withColumn(\"sorted\", sort_array(\"array\", asc=False))\n",
    " .withColumn(\"size\", size(\"array\"))\n",
    " .withColumn(\"contains\", array_contains(\"array\", \"c\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88bf765c-b56e-4162-9373-a1188ffcb33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+---+\n",
      "| _1|test| array| yo|\n",
      "+---+----+------+---+\n",
      "|  1| a_b|[a, b]|  a|\n",
      "|  1| a_b|[a, b]|  b|\n",
      "+---+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.withColumn(\"test\", lit(\"a_b\"))\n",
    " .withColumn(\"array\", split(\"test\", \"_\"))\n",
    " .withColumn(\"yo\", explode(\"array\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f2e09-01f5-453c-a2ee-50aecbda0493",
   "metadata": {},
   "source": [
    "## Manpulating Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3181500b-35e0-4137-89ff-24fffe389fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|name|upper|\n",
      "+----+-----+\n",
      "|   a|    A|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('a',)], 'name string')\n",
    "\n",
    "df.withColumn(\"upper\", upper('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e638926a-5cef-45b7-80ab-bb9be20e6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------+\n",
      "|name|         description| result|\n",
      "+----+--------------------+-------+\n",
      "|   a|Description: bla bla|bla bla|\n",
      "+----+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.withColumn(\"description\", lit(\"Description: bla bla\"))\n",
    " .withColumn(\"result\", regexp_replace(\"description\", \"^Description: \", \"\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f45604-047b-4098-8f31-2dc58fdc3334",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be1687ec-7699-4405-9e95-15d06330f19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|name|test|\n",
      "+----+----+\n",
      "|   a|  yo|\n",
      "|   b|  yo|\n",
      "+----+----+\n",
      "\n",
      "+----+----+\n",
      "|name|test|\n",
      "+----+----+\n",
      "|   a|  yo|\n",
      "+----+----+\n",
      "\n",
      "+----+\n",
      "|test|\n",
      "+----+\n",
      "|  yo|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('a',), ('b',)], 'name string')\n",
    "\n",
    "df_with_duplicates = df.withColumn(\"test\", lit(\"yo\"))\n",
    "df_with_duplicates.show()\n",
    "\n",
    "df_with_duplicates.dropDuplicates([\"test\"]).show()\n",
    "\n",
    "df_with_duplicates.select(\"test\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0880e9f-f94a-4076-ae0c-887dc72cb63d",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a7d2e8e5-adf8-4f21-8855-7b29481f9666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|mean|\n",
      "+----+\n",
      "| 4.5|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(10)\n",
    "\n",
    "df.agg(mean(\"id\").alias(\"mean\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e65f31c9-2f73-4fcc-b70f-db4fea9a98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distincCount|\n",
      "+------------+\n",
      "|          11|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(approx_count_distinct(col(\"id\"), 0.15).alias(\"distincCount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ddd92e9-ab8c-4380-8916-23ee26f09784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|highest|lowest|\n",
      "+---+-------+------+\n",
      "|  B|     30|    15|\n",
      "|  A|    100|     3|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"A\", 3), (\"B\", 30), (\"B\", 15), (\"A\", 100)], [\"id\", \"value\"])\n",
    "\n",
    "df.groupBy(\"id\").agg(max(\"value\").alias(\"highest\"), min(\"value\").alias(\"lowest\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "10823065-3919-47bd-b576-10da7ef5d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+------+\n",
      "|year|dotNET| Java|Kotlin|\n",
      "+----+------+-----+------+\n",
      "|2012| 15000|20000|  null|\n",
      "|2013| 48000|30000|  1000|\n",
      "+----+------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    Row(course=\"dotNET\", year=2012, earnings=10000),\n",
    "    Row(course=\"Java\", year=2012, earnings=20000),\n",
    "    Row(course=\"dotNET\", year=2012, earnings=5000),\n",
    "    Row(course=\"dotNET\", year=2013, earnings=48000),\n",
    "    Row(course=\"Java\", year=2013, earnings=30000),\n",
    "    Row(course=\"Kotlin\", year=2013, earnings=1000),\n",
    "    Row(course=\"Javascript\", year=2013, earnings=100),\n",
    "])\n",
    "df1.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\", \"Kotlin\"]).sum(\"earnings\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684773e3-a407-465f-b03f-19a6aaace3ef",
   "metadata": {},
   "source": [
    "## Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9742c2d-1eaf-49af-befe-4c2197cf2924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-------+----+\n",
      "|age|  country|      date|   name|rank|\n",
      "+---+---------+----------+-------+----+\n",
      "| 50|Allemagne|2023-10-10|Richard|   1|\n",
      "| 40|   France|2023-10-01|  Alice|   1|\n",
      "| 28|   France|2023-10-01|   Jane|   2|\n",
      "| 30|   France|2023-10-10|    Bob|   1|\n",
      "| 20|   Italie|2023-10-26|   Omar|   1|\n",
      "+---+---------+----------+-------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "d = [\n",
    "    {'name': 'Alice', 'age': 40, 'country': 'France', 'date': \"2023-10-01\"},\n",
    "    {'name': 'Jane', 'age': 28, 'country': 'France', 'date': \"2023-10-01\"},\n",
    "    {'name': 'Bob', 'age': 30, 'country': 'France', 'date': \"2023-10-10\"},\n",
    "    {'name': 'Richard', 'age': 50, 'country': 'Allemagne', 'date': \"2023-10-10\"},\n",
    "    {'name': 'Omar', 'age': 20, 'country': 'Italie', 'date': \"2023-10-26\"}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(d).withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "window = (Window\n",
    "          .partitionBy(\"country\", \"date\")\n",
    "          .orderBy(desc(\"age\"))\n",
    "          .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "df.withColumn(\"rank\", rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7abfe-96f6-4c49-a2a2-9c8d9806fd28",
   "metadata": {},
   "source": [
    "## Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6197c037-be2f-4201-8d25-e7c42fb448d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(2, 20), (3, 30), (1, 10), (1, 30), (1, 20)], [\"a\", \"b\"])\n",
    "\n",
    "assert df.orderBy(\"a\", ascending=False).select(\"a\").first().a == 3\n",
    "assert df.orderBy([\"b\", \"a\"]).first() == Row(1, 10)\n",
    "assert df.sort(col(\"a\").desc()).first().b == 30\n",
    "assert df.sort(desc(\"a\")).first().a == 3\n",
    "assert df.sort(\"a\", desc(\"b\")).first() == Row(1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec3f13-0000-4532-89c7-8e209d4dea75",
   "metadata": {},
   "source": [
    "## Describe dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0bc6403-df76-4186-b769-0fbc51cd4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                 a|\n",
      "+-------+------------------+\n",
      "|  count|                 5|\n",
      "|   mean|               1.6|\n",
      "| stddev|0.8944271909999159|\n",
      "|    min|                 1|\n",
      "|    max|                 3|\n",
      "+-------+------------------+\n",
      "\n",
      "+-------+---+---+\n",
      "|summary|  a|  b|\n",
      "+-------+---+---+\n",
      "|  count|  5|  5|\n",
      "+-------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(2, 20), (3, 30), (1, 10), (1, 30), (1, 20)], [\"a\", \"b\"])\n",
    "\n",
    "df.describe(\"a\").show()\n",
    "df.summary(\"count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c72fd-74d7-406c-bf3f-496423fc30cf",
   "metadata": {},
   "source": [
    "## Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e874647f-927b-41f6-b23c-2c4907262540",
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = [1, 2, 2, 3, 4, 4, 5]\n",
    "dfInt = spark.createDataFrame(integers, IntegerType())\n",
    "\n",
    "assert [row.value for row in dfInt.sample(True, fraction=0.5, seed=3).collect()] == [4, 5]\n",
    "assert [row.value for row in dfInt.sample(False, fraction=0.5, seed=3).collect()] == [1, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5efff-d91a-4ed6-abfd-f509f507aa41",
   "metadata": {},
   "source": [
    "## Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb1bce9e-3ca2-4967-8246-c456bf1dedb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+-----+-----------+------------------------------+------------+\n",
      "|_1 |_2 |test               |month|day_of_year|yo                            |yo_timestamp|\n",
      "+---+---+-------------------+-----+-----------+------------------------------+------------+\n",
      "|1  |a  |2014-08-14 16:03:17|8    |226        |Thursday, Aug 14, 2014 4:03 PM|1704188880  |\n",
      "|2  |b  |2014-08-14 16:03:17|8    |226        |Thursday, Aug 14, 2014 4:03 PM|1704188880  |\n",
      "+---+---+-------------------+-----+-----------+------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\")])\n",
    "(df\n",
    " .withColumn(\"test\", lit(1408024997).cast(\"timestamp\"))\n",
    " .withColumn(\"month\", month(col(\"test\")))\n",
    " .withColumn(\"day_of_year\", dayofyear(col(\"test\")))\n",
    " .withColumn(\"yo\", from_unixtime(lit(1408024997), \"EEEE, MMM d, yyyy h:mm a\"))\n",
    " .withColumn(\"yo_timestamp\", unix_timestamp(lit(\"02/01/2024 10:48\"), \"dd/MM/yyyy HH:mm\"))\n",
    " .show(10, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "680082e3-4fcc-49cd-b517-964bd8f97f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----------+\n",
      "|               date|     date_timestamp| date_date|\n",
      "+-------------------+-------------------+----------+\n",
      "|23/01/2022 11:28:12|2022-01-23 11:28:12|2022-01-23|\n",
      "|24/01/2022 10:58:34|2022-01-24 10:58:34|2022-01-24|\n",
      "+-------------------+-------------------+----------+\n",
      "\n",
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_timestamp: timestamp (nullable = true)\n",
      " |-- date_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDates = spark.createDataFrame([(\"23/01/2022 11:28:12\",),(\"24/01/2022 10:58:34\",)], [\"date\"])\n",
    "dfDates = (\n",
    "    dfDates.withColumn(\"date_timestamp\", to_timestamp(\"date\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "    .withColumn(\"date_date\", to_date(\"date\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    ")\n",
    "\n",
    "dfDates.show()\n",
    "dfDates.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2072c5-8af8-40e5-8a65-9f04511a8a33",
   "metadata": {},
   "source": [
    "## Fill with empty values (na.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f292fc1-8a3e-4554-ac66-06d19053adb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------+---------+\n",
      "| id|test_na_1|test_na_2|test_na_3|test_na_4|\n",
      "+---+---------+---------+---------+---------+\n",
      "|  0|       yo|       30|     null|        5|\n",
      "|  1|       yo|       30|     null|        5|\n",
      "+---+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(2)\n",
    "(\n",
    "    df.withColumn(\"test_na_1\", lit(None).cast(StringType()))\n",
    "    .withColumn(\"test_na_2\", lit(None).cast(IntegerType()))\n",
    "    .withColumn(\"test_na_3\", lit(None).cast(IntegerType()))\n",
    "    .withColumn(\"test_na_4\", lit(None).cast(IntegerType()))\n",
    "    .na.fill(\"yo\")\n",
    "    .na.fill({'test_na_2': 30})\n",
    "    .na.fill(5, \"test_na_4\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04500596-2478-4407-bc3a-e1de31738bb4",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aa3a20d5-e8f5-4b00-a7a0-c658262a7508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+---+-------+\n",
      "|  name| id|   name| id|dept_id|\n",
      "+------+---+-------+---+-------+\n",
      "| Alice|  1| Franck|  3|     42|\n",
      "| Alice|  1|Bernard|  4|     42|\n",
      "| Alice|  1|  Ramzy|  5|     43|\n",
      "|   Bob|  2| Franck|  3|     42|\n",
      "|   Bob|  2|Bernard|  4|     42|\n",
      "|   Bob|  2|  Ramzy|  5|     43|\n",
      "|Rachid|  3| Franck|  3|     42|\n",
      "|Rachid|  3|Bernard|  4|     42|\n",
      "|Rachid|  3|  Ramzy|  5|     43|\n",
      "+------+---+-------+---+-------+\n",
      "\n",
      "+------+---+------+---+-------+\n",
      "|  name| id|  name| id|dept_id|\n",
      "+------+---+------+---+-------+\n",
      "|Rachid|  3|Franck|  3|     42|\n",
      "+------+---+------+---+-------+\n",
      "\n",
      "+---+-------+-------+\n",
      "| id|   name|dept_id|\n",
      "+---+-------+-------+\n",
      "|  1|  Alice|   null|\n",
      "|  2|    Bob|   null|\n",
      "|  3| Franck|     42|\n",
      "|  3| Rachid|   null|\n",
      "|  4|Bernard|     42|\n",
      "|  5|  Ramzy|     43|\n",
      "+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2), (\"Rachid\", 3)], \"name:string, id: int\")\n",
    "b = spark.createDataFrame([(\"Franck\", 3, 42), (\"Bernard\", 4, 42), (\"Ramzy\", 5, 43)], \"name:string, id: int, dept_id: int\")\n",
    "\n",
    "a.crossJoin(b).show()\n",
    "\n",
    "a.join(b, a.id == b.id).show()\n",
    "a.join(b, [a.id == b.id])\n",
    "a.join(b, [\"id\", \"name\"])\n",
    "a.alias(\"a\").join(b.alias(\"b\"), [col(\"a.id\") == col(\"b.id\"), col(\"a.name\") == col(\"b.name\")])\n",
    "a.join(b.select(\"dept_id\"), col(\"id\") == col(\"dept_id\"))\n",
    "\n",
    "a.join(b, [\"id\", \"name\"], \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "019dc7c6-eeee-459c-a63b-9ad205509f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_df = spark.createDataFrame([1, 2, 3], IntegerType())\n",
    "large_df = spark.createDataFrame([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], IntegerType())\n",
    "\n",
    "large_df.join(broadcast(small_df), \"v\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a2ba0-e472-4eb8-9b1e-a2c5b1bc605a",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c76f49b8-1c43-4c32-a986-ca997b30eeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  Alice|  1|\n",
      "|    Bob|  2|\n",
      "| Rachid|  3|\n",
      "| Franck| 30|\n",
      "|Bernard| 40|\n",
      "|  Ramzy| 50|\n",
      "+-------+---+\n",
      "\n",
      "+-------+----+----+\n",
      "|   name|  id| age|\n",
      "+-------+----+----+\n",
      "|  Alice|   1|null|\n",
      "|    Bob|   2|null|\n",
      "| Rachid|   3|null|\n",
      "| Franck|null|  30|\n",
      "|Bernard|null|  40|\n",
      "|  Ramzy|null|  50|\n",
      "+-------+----+----+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  Alice|\n",
      "|    Bob|\n",
      "| Rachid|\n",
      "| Franck|\n",
      "|Bernard|\n",
      "|  Ramzy|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2), (\"Rachid\", 3)], \"name:string, id: int\")\n",
    "b = spark.createDataFrame([(\"Franck\", 30), (\"Bernard\", 40), (\"Ramzy\", 50)], \"name:string, age: int\")\n",
    "\n",
    "a.union(b).show()\n",
    "a.unionByName(b, allowMissingColumns=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f97d19b-e0ae-4ae3-b1cb-d019370f0777",
   "metadata": {},
   "source": [
    "## Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33a51720-6f4d-4fc4-a0d9-7f2c56493e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- test: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- test: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"test\", col(\"_1\").cast(StringType())).printSchema()\n",
    "df.withColumn(\"test\", col(\"_1\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db27a9-bf0d-4451-8df0-66e421058fd3",
   "metadata": {},
   "source": [
    "## UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d36ae886-3022-449f-b3cd-d1c9b11450ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/23 22:38:23 WARN SimpleFunctionRegistry: The function add_42 replaced a previously registered function.\n",
      "+----+----------+\n",
      "|  _1|ADD_42(_1)|\n",
      "+----+----------+\n",
      "|   1|        43|\n",
      "|null|      null|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,), (None,)])\n",
    "\n",
    "def add42(n):\n",
    "    if n is None:\n",
    "        return\n",
    "    return n + 42\n",
    "    \n",
    "df.createOrReplaceTempView(\"test\")\n",
    "\n",
    "spark.udf.register(\"ADD_42\", add42)\n",
    "\n",
    "spark.sql(\"select _1, ADD_42(_1) from test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "366aaf84-9999-47c2-ab54-e3637d30f82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|  _1|test|\n",
      "+----+----+\n",
      "|   1|  10|\n",
      "|null|null|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,), (None,)])\n",
    "\n",
    "def multiply_by_10(n):\n",
    "    if n is None:\n",
    "        return\n",
    "    return n * 10\n",
    "\n",
    "multiply_by_10_UDF = udf(multiply_by_10, IntegerType())\n",
    "\n",
    "df.withColumn(\"test\", multiply_by_10_UDF(\"_1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6e72f-0496-4fbc-b7a0-18132e246059",
   "metadata": {},
   "source": [
    "## Cache and Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad73f404-a4e8-4b53-b642-3088ce680fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Deserialized 1x Replicated\n",
      "Memory Serialized 1x Replicated\n",
      "23/12/22 22:11:11 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# cache in MEMORY_AND_DISK_DESER (default)\n",
    "df.cache().count()\n",
    "assert str(df.storageLevel).startswith(\"Disk\") == True\n",
    "print(df.storageLevel)\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "# cache in MEMORY_ONLY\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "assert str(df.storageLevel).startswith(\"Memory\") == True\n",
    "print(df.storageLevel)\n",
    "\n",
    "# stores dataframe on two different executors, utilizing the executors' memory as much as possible, but not writing anything to disk.\n",
    "df.persist(StorageLevel.MEMORY_ONLY_2).count()\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "assert df.is_cached == False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cdca1-7ea9-47d6-ac25-5aef744d594f",
   "metadata": {},
   "source": [
    "## Write dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bf17c762-dddf-4da1-908a-69c32d809e4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot use all columns for partition columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [82], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[0;32m----> 2\u001b[0m     df\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mwrite\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mparquet(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/krebai/tmp\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/readwriter.py:885\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m--> 885\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot use all columns for partition columns"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df\n",
    "    .write\n",
    "    .partitionBy(\"_1\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"/home/krebai/tmp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5af6b796-f9e5-4a7a-9f0e-bf59292994e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").json(\"/home/krebai/tmp/json\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"_1\", LongType(), True),\n",
    "    StructField(\"_2\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.read.json(\"/home/krebai/tmp/json\", schema=schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ff9669-1b2b-4a4d-9c28-64b5d1d84178",
   "metadata": {},
   "source": [
    "## Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6352f124-1768-46f9-ac10-0989e613df0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([('yo Inc.',), ('test',), ('Inc. fada',)], 'name: string')\n",
    "\n",
    "accum = sc.accumulator(0)\n",
    "\n",
    "def check(row):\n",
    "    if 'Inc.' in row['name']:\n",
    "        accum.add(1)\n",
    "\n",
    "df.foreach(check)\n",
    "print(accum.value)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7901a-174b-48a3-95b4-2d9ccc485912",
   "metadata": {},
   "source": [
    "## Compute number of business days between 2 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "87b0ce11-fc14-450d-a68e-88b4180d5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_facts = spark.createDataFrame(\n",
    "    [('data1', '2023-12-18', '2023-12-24'),\n",
    "     ('data1', '2022-05-08', '2022-05-21')],\n",
    "    ['data', 'start_date', 'end_date']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f938056b-602d-468d-a589-1f0c05283ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+---------------+---------------+\n",
      "| data|start_date|  end_date|business_days_1|business_days_2|\n",
      "+-----+----------+----------+---------------+---------------+\n",
      "|data1|2023-12-18|2023-12-24|              5|              5|\n",
      "|data1|2022-05-08|2022-05-21|             10|             10|\n",
      "+-----+----------+----------+---------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_facts.withColumn(\"test\", sequence(to_date(\"start_date\"), to_date(\"end_date\")))\n",
    "    .withColumn(\"business_days_1\", expr(\"size(filter(test, x -> dayofweek(x) != 1 and dayofweek(x) != 7))\"))\n",
    "    .withColumn(\"business_days_2\", size(filter(\"test\", lambda x: (dayofweek(x) != 1) & (dayofweek(x) != 7))))\n",
    "    .drop(\"test\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bc89b-1c16-4d34-85e1-802140277011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
