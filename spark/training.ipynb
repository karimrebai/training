{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed105df-2ac1-46f8-b48d-860af4fb3954",
   "metadata": {},
   "source": [
    "## Create SparkSession and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb14e8a8-a5a3-497a-8836-f809b2dceac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/03 12:12:19 WARN Utils: Your hostname, krxps resolves to a loopback address: 127.0.1.1; using 192.168.68.61 instead (on interface wlp0s20f3)\n",
      "24/01/03 12:12:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/03 12:12:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import Window\n",
    "\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcdba2d-228e-4e2f-8342-f1482e4729f5",
   "metadata": {},
   "source": [
    "## Create a dataframe (with SparkSession)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f34fb72-5c62-4475-8417-c22505e391a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  2|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1,), (2,)], \"id: int\")\n",
    "df.show()\n",
    "\n",
    "df.createOrReplaceTempView(\"test\")\n",
    "\n",
    "df_table = spark.table(\"test\")\n",
    "df_table.show()\n",
    "\n",
    "df_sql = spark.sql(\"select id from test where id = 2\")\n",
    "df_sql.show()\n",
    "\n",
    "df_range = spark.range(0, 3)\n",
    "df_range.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98975c23-ce58-4f90-a8c2-299bee652e36",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa08828d-6b90-4479-8878-de5e1416cf84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2='a')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\")])\n",
    "\n",
    "df.filter((col(\"_1\") == 1) | (col(\"_2\") == \"a\")).take(5)\n",
    "\n",
    "df.filter(\"_1 == 1 or _2 == 'a'\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9433384e-992a-48ac-a15b-9e3e00e60b69",
   "metadata": {},
   "source": [
    "## Create column / Math function / Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48bd7e71-f595-4c66-acdc-e1a74302827a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "| _1| _2|squared|\n",
      "+---+---+-------+\n",
      "|  1|  a|    1.0|\n",
      "|  2|  b|    4.0|\n",
      "+---+---+-------+\n",
      "\n",
      "+---+---+-------+\n",
      "| _1| _2|squared|\n",
      "+---+---+-------+\n",
      "|  1|  a|    1.0|\n",
      "|  2|  b|    4.0|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"squared\", pow(\"_1\", lit(2))).show()\n",
    "\n",
    "df.withColumn(\"squared\", pow(\"_1\", 2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "82c60c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|boolean|\n",
      "+-------+\n",
      "|  false|\n",
      "|   true|\n",
      "|   true|\n",
      "|   true|\n",
      "|  false|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(10, \"a\"), (20, \"b\"), (30, \"c\"), (40, \"d\"), (40, \"e\")], [\"id\", \"name\"])\n",
    "\n",
    "df.select((col(\"id\").between(20, 40) & col(\"name\").isin(\"b\", \"c\", \"d\")).alias(\"boolean\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da20b34-c3e2-4960-a1b8-259e20d8a03a",
   "metadata": {},
   "source": [
    "# Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "592dbe87-4067-4e3d-a308-c12f642254fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+------+--------------+------+----+--------+\n",
      "|  a|  b|test| array|second_element|sorted|size|contains|\n",
      "+---+---+----+------+--------------+------+----+--------+\n",
      "|  2| 20| a_b|[a, b]|             b|[b, a]|   2|   false|\n",
      "|  3| 30| a_b|[a, b]|             b|[b, a]|   2|   false|\n",
      "|  1| 10| a_b|[a, b]|             b|[b, a]|   2|   false|\n",
      "|  1| 30| a_b|[a, b]|             b|[b, a]|   2|   false|\n",
      "|  1| 20| a_b|[a, b]|             b|[b, a]|   2|   false|\n",
      "+---+---+----+------+--------------+------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df\n",
    " .withColumn(\"test\", lit(\"a_b\"))\n",
    " .withColumn(\"array\", split(col(\"test\"), \"_\"))\n",
    " .withColumn(\"second_element\", col(\"array\")[1])\n",
    " .withColumn(\"sorted\", sort_array(\"array\", asc=False))\n",
    " .withColumn(\"size\", size(\"array\"))\n",
    " .withColumn(\"contains\", array_contains(\"array\", \"c\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88bf765c-b56e-4162-9373-a1188ffcb33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+------+---+\n",
      "| _1| _2|test| array| yo|\n",
      "+---+---+----+------+---+\n",
      "|  1|  a| a_b|[a, b]|  a|\n",
      "|  1|  a| a_b|[a, b]|  b|\n",
      "|  2|  b| a_b|[a, b]|  a|\n",
      "|  2|  b| a_b|[a, b]|  b|\n",
      "+---+---+----+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.withColumn(\"test\", lit(\"a_b\"))\n",
    " .withColumn(\"array\", split(\"test\", \"_\"))\n",
    " .withColumn(\"yo\", explode(col(\"array\")))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f2e09-01f5-453c-a2ee-50aecbda0493",
   "metadata": {},
   "source": [
    "# Manpulate strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3181500b-35e0-4137-89ff-24fffe389fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+\n",
      "| _1| _2|upper|\n",
      "+---+---+-----+\n",
      "|  1|  a|    A|\n",
      "|  2|  b|    B|\n",
      "+---+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"upper\", upper(col(\"_2\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e638926a-5cef-45b7-80ab-bb9be20e6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+-------+\n",
      "| _1| _2|         description| result|\n",
      "+---+---+--------------------+-------+\n",
      "|  1|  a|Description: bla bla|bla bla|\n",
      "|  2|  b|Description: bla bla|bla bla|\n",
      "+---+---+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.withColumn(\"description\", lit(\"Description: bla bla\"))\n",
    " .withColumn(\"result\", regexp_replace(\"description\", \"^Description: \", \"\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d805489-b2d2-429b-80c3-73573a48ec0b",
   "metadata": {},
   "source": [
    "# Rename a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "72cdfda2-3059-456f-abfd-2a05338342fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "| id| _2|name|\n",
      "+---+---+----+\n",
      "|  1|  a|   a|\n",
      "|  2|  b|   b|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.withColumnRenamed(\"_1\", \"id\")\n",
    " .withColumn(\"name\", col(\"_2\"))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f45604-047b-4098-8f31-2dc58fdc3334",
   "metadata": {},
   "source": [
    "# Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be1687ec-7699-4405-9e95-15d06330f19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|  a|  b|test|\n",
      "+---+---+----+\n",
      "|  2| 20|  yo|\n",
      "|  3| 30|  yo|\n",
      "|  1| 10|  yo|\n",
      "|  1| 30|  yo|\n",
      "|  1| 20|  yo|\n",
      "+---+---+----+\n",
      "\n",
      "+---+---+----+\n",
      "|  a|  b|test|\n",
      "+---+---+----+\n",
      "|  2| 20|  yo|\n",
      "+---+---+----+\n",
      "\n",
      "+----+\n",
      "|test|\n",
      "+----+\n",
      "|  yo|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_duplicates = df.withColumn(\"test\", lit(\"yo\"))\n",
    "df_with_duplicates.show()\n",
    "\n",
    "df_with_duplicates.drop_duplicates(subset = [\"test\"]).show()\n",
    "\n",
    "df_with_duplicates.select(\"test\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0880e9f-f94a-4076-ae0c-887dc72cb63d",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a7d2e8e5-adf8-4f21-8855-7b29481f9666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|mean|\n",
      "+----+\n",
      "| 1.5|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(mean(col(\"_1\")).alias(\"mean\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e65f31c9-2f73-4fcc-b70f-db4fea9a98a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|distincCount|\n",
      "+------------+\n",
      "|           2|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(approx_count_distinct(col(\"_1\"), 0.15).alias(\"distincCount\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ddd92e9-ab8c-4380-8916-23ee26f09784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|highest|lowest|\n",
      "+---+-------+------+\n",
      "|  B|     30|    15|\n",
      "|  A|    100|     3|\n",
      "+---+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(\"A\", 3), (\"B\", 30), (\"B\", 15), (\"A\", 100)], [\"id\", \"value\"])\n",
    "\n",
    "df.groupBy(\"id\").agg(max(\"value\").alias(\"highest\"), min(\"value\").alias(\"lowest\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10823065-3919-47bd-b576-10da7ef5d858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|year|dotNET| Java|\n",
      "+----+------+-----+\n",
      "|2012| 15000|20000|\n",
      "|2013| 48000|30000|\n",
      "+----+------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    Row(course=\"dotNET\", year=2012, earnings=10000),\n",
    "    Row(course=\"Java\", year=2012, earnings=20000),\n",
    "    Row(course=\"dotNET\", year=2012, earnings=5000),\n",
    "    Row(course=\"dotNET\", year=2013, earnings=48000),\n",
    "    Row(course=\"Java\", year=2013, earnings=30000),\n",
    "])\n",
    "df1.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684773e3-a407-465f-b03f-19a6aaace3ef",
   "metadata": {},
   "source": [
    "## Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9742c2d-1eaf-49af-befe-4c2197cf2924",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-------+----+\n",
      "|age|  country|      date|   name|rank|\n",
      "+---+---------+----------+-------+----+\n",
      "| 50|Allemagne|2023-10-10|Richard|   1|\n",
      "| 40|   France|2023-10-01|  Alice|   1|\n",
      "| 28|   France|2023-10-01|   Jane|   2|\n",
      "| 30|   France|2023-10-10|    Bob|   1|\n",
      "| 20|   Italie|2023-10-26|   Omar|   1|\n",
      "+---+---------+----------+-------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "d = [\n",
    "    {'name': 'Alice', 'age': 40, 'country': 'France', 'date': \"2023-10-01\"},\n",
    "    {'name': 'Jane', 'age': 28, 'country': 'France', 'date': \"2023-10-01\"},\n",
    "    {'name': 'Bob', 'age': 30, 'country': 'France', 'date': \"2023-10-10\"},\n",
    "    {'name': 'Richard', 'age': 50, 'country': 'Allemagne', 'date': \"2023-10-10\"},\n",
    "    {'name': 'Omar', 'age': 20, 'country': 'Italie', 'date': \"2023-10-26\"}\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(d).withColumn(\"date\", to_date(\"date\"))\n",
    "\n",
    "window = (Window\n",
    "          .partitionBy(\"country\", \"date\")\n",
    "          .orderBy(desc(\"age\"))\n",
    "          .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    ")\n",
    "df.withColumn(\"rank\", rank().over(window)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7abfe-96f6-4c49-a2a2-9c8d9806fd28",
   "metadata": {},
   "source": [
    "# Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6197c037-be2f-4201-8d25-e7c42fb448d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|  1| 30|\n",
      "|  1| 20|\n",
      "|  1| 10|\n",
      "|  2| 20|\n",
      "|  3| 30|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(2, 20), (3, 30), (1, 10), (1, 30), (1, 20)], [\"a\", \"b\"])\n",
    "\n",
    "assert df.orderBy(\"a\", ascending=False).select(\"a\").first().a == 3\n",
    "assert df.sort(col(\"a\").desc()).first().b == 30\n",
    "assert df.sort(desc(\"a\")).first().a == 3\n",
    "assert df.sort(\"a\", desc(\"b\")).first() == Row(1, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec3f13-0000-4532-89c7-8e209d4dea75",
   "metadata": {},
   "source": [
    "# Describe dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0bc6403-df76-4186-b769-0fbc51cd4a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|                _1|\n",
      "+-------+------------------+\n",
      "|  count|                 2|\n",
      "|   mean|               1.5|\n",
      "| stddev|0.7071067811865476|\n",
      "|    min|                 1|\n",
      "|    max|                 2|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(\"_1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318c72fd-74d7-406c-bf3f-496423fc30cf",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e874647f-927b-41f6-b23c-2c4907262540",
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = [1, 2, 2, 3, 4, 4, 5]\n",
    "dfInt = spark.createDataFrame(integers, IntegerType())\n",
    "\n",
    "assert [row.value for row in dfInt.sample(True, fraction=0.5, seed=3).collect()] == [4, 5]\n",
    "assert [row.value for row in dfInt.sample(False, fraction=0.5, seed=3).collect()] == [1, 4, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a5efff-d91a-4ed6-abfd-f509f507aa41",
   "metadata": {},
   "source": [
    "# Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb1bce9e-3ca2-4967-8246-c456bf1dedb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+-----+-----------+------------------------------+------------+\n",
      "|_1 |_2 |test               |month|day_of_year|yo                            |yo_timestamp|\n",
      "+---+---+-------------------+-----+-----------+------------------------------+------------+\n",
      "|1  |a  |2014-08-14 16:03:17|8    |226        |Thursday, Aug 14, 2014 4:03 PM|1704188880  |\n",
      "|2  |b  |2014-08-14 16:03:17|8    |226        |Thursday, Aug 14, 2014 4:03 PM|1704188880  |\n",
      "+---+---+-------------------+-----+-----------+------------------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, \"a\"), (2, \"b\")])\n",
    "(\n",
    "    df.withColumn(\"test\", lit(1408024997).cast(\"timestamp\"))\n",
    "    .withColumn(\"month\", month(col(\"test\")))\n",
    "    .withColumn(\"day_of_year\", dayofyear(col(\"test\")))\n",
    "    .withColumn(\"yo\", from_unixtime(lit(1408024997), \"EEEE, MMM d, yyyy h:mm a\"))\n",
    "    .withColumn(\"yo_timestamp\", unix_timestamp(lit(\"02/01/2024 10:48\"), \"dd/MM/yyyy HH:mm\"))\n",
    "    .show(10, False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680082e3-4fcc-49cd-b517-964bd8f97f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDates = spark.createDataFrame([(\"23/01/2022 11:28:12\",),(\"24/01/2022 10:58:34\",)], [\"date\"])\n",
    "dfDates = dfDates.withColumn(\"date\", to_timestamp(\"date\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "dfDates.show()\n",
    "dfDates.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2072c5-8af8-40e5-8a65-9f04511a8a33",
   "metadata": {},
   "source": [
    "# Fill with empty values (na.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0f292fc1-8a3e-4554-ac66-06d19053adb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+---------+---------+---------+\n",
      "| _1| _2|test_na_1|test_na_2|test_na_3|test_na_4|\n",
      "+---+---+---------+---------+---------+---------+\n",
      "|  1|  a|       yo|       30|     null|        5|\n",
      "|  2|  b|       yo|       30|     null|        5|\n",
      "+---+---+---------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df.withColumn(\"test_na_1\", lit(None).cast(StringType()))\n",
    "    .withColumn(\"test_na_2\", lit(None).cast(IntegerType()))\n",
    "    .withColumn(\"test_na_3\", lit(None).cast(IntegerType()))\n",
    "    .withColumn(\"test_na_4\", lit(None).cast(IntegerType()))\n",
    "    .na.fill(\"yo\")\n",
    "    .na.fill({'test_na_2': 30})\n",
    "    .na.fill(5, \"test_na_4\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04500596-2478-4407-bc3a-e1de31738bb4",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa3a20d5-e8f5-4b00-a7a0-c658262a7508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-------+---+-------+\n",
      "|  name| id|   name| id|dept_id|\n",
      "+------+---+-------+---+-------+\n",
      "| Alice|  1| Franck|  3|     42|\n",
      "| Alice|  1|Bernard|  4|     42|\n",
      "| Alice|  1|  Ramzy|  5|     43|\n",
      "|   Bob|  2| Franck|  3|     42|\n",
      "|   Bob|  2|Bernard|  4|     42|\n",
      "|   Bob|  2|  Ramzy|  5|     43|\n",
      "|Rachid|  3| Franck|  3|     42|\n",
      "|Rachid|  3|Bernard|  4|     42|\n",
      "|Rachid|  3|  Ramzy|  5|     43|\n",
      "+------+---+-------+---+-------+\n",
      "\n",
      "+------+---+------+---+-------+\n",
      "|  name| id|  name| id|dept_id|\n",
      "+------+---+------+---+-------+\n",
      "|Rachid|  3|Franck|  3|     42|\n",
      "+------+---+------+---+-------+\n",
      "\n",
      "+---+-------+-------+\n",
      "| id|   name|dept_id|\n",
      "+---+-------+-------+\n",
      "|  1|  Alice|   null|\n",
      "|  2|    Bob|   null|\n",
      "|  3| Franck|     42|\n",
      "|  3| Rachid|   null|\n",
      "|  4|Bernard|     42|\n",
      "|  5|  Ramzy|     43|\n",
      "+---+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2), (\"Rachid\", 3)], \"name:string, id: int\")\n",
    "b = spark.createDataFrame([(\"Franck\", 3, 42), (\"Bernard\", 4, 42), (\"Ramzy\", 5, 43)], \"name:string, id: int, dept_id: int\")\n",
    "\n",
    "a.crossJoin(b).show()\n",
    "\n",
    "a.join(b, a.id == b.id).show()\n",
    "a.join(b, [a.id == b.id])\n",
    "a.alias(\"a\").join(b.alias(\"b\"), [col(\"a.id\") == col(\"b.id\"), col(\"a.name\") == col(\"b.name\")])\n",
    "a.join(b.select(\"dept_id\"), col(\"id\") == col(\"dept_id\"))\n",
    "\n",
    "a.join(b, [\"id\", \"name\"], \"outer\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "019dc7c6-eeee-459c-a63b-9ad205509f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_df = spark.createDataFrame([1, 2, 3], IntegerType())\n",
    "large_df = spark.createDataFrame([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], IntegerType())\n",
    "\n",
    "large_df.join(broadcast(small_df), \"v\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a2ba0-e472-4eb8-9b1e-a2c5b1bc605a",
   "metadata": {},
   "source": [
    "# Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c76f49b8-1c43-4c32-a986-ca997b30eeec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name| id|\n",
      "+-------+---+\n",
      "|  Alice|  1|\n",
      "|    Bob|  2|\n",
      "| Rachid|  3|\n",
      "| Franck| 30|\n",
      "|Bernard| 40|\n",
      "|  Ramzy| 50|\n",
      "+-------+---+\n",
      "\n",
      "+-------+----+----+\n",
      "|   name|  id| age|\n",
      "+-------+----+----+\n",
      "|  Alice|   1|null|\n",
      "|    Bob|   2|null|\n",
      "| Rachid|   3|null|\n",
      "| Franck|null|  30|\n",
      "|Bernard|null|  40|\n",
      "|  Ramzy|null|  50|\n",
      "+-------+----+----+\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|  Alice|\n",
      "|    Bob|\n",
      "| Rachid|\n",
      "| Franck|\n",
      "|Bernard|\n",
      "|  Ramzy|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = spark.createDataFrame([(\"Alice\", 1), (\"Bob\", 2), (\"Rachid\", 3)], \"name:string, id: int\")\n",
    "b = spark.createDataFrame([(\"Franck\", 30), (\"Bernard\", 40), (\"Ramzy\", 50)], \"name:string, age: int\")\n",
    "\n",
    "a.union(b).show()\n",
    "a.unionByName(b, allowMissingColumns=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f97d19b-e0ae-4ae3-b1cb-d019370f0777",
   "metadata": {},
   "source": [
    "# Cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33a51720-6f4d-4fc4-a0d9-7f2c56493e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- test: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- test: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"test\", col(\"_1\").cast(StringType())).printSchema()\n",
    "df.withColumn(\"test\", col(\"_1\").cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db27a9-bf0d-4451-8df0-66e421058fd3",
   "metadata": {},
   "source": [
    "# UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d36ae886-3022-449f-b3cd-d1c9b11450ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/13 22:06:01 WARN SimpleFunctionRegistry: The function add_42 replaced a previously registered function.\n",
      "+----------+---+\n",
      "|ADD_42(_1)| _1|\n",
      "+----------+---+\n",
      "|        43|  1|\n",
      "|        44|  2|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def add42(n):\n",
    "    return n + 42\n",
    "    \n",
    "df.createOrReplaceTempView(\"test\")\n",
    "\n",
    "spark.udf.register(\"ADD_42\", add42)\n",
    "\n",
    "spark.sql(\"select ADD_42(_1), _1 from test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "366aaf84-9999-47c2-ab54-e3637d30f82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "| _1| _2|test|\n",
      "+---+---+----+\n",
      "|  1|  a|  10|\n",
      "|  2|  b|  20|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def multiply_by_10(n):\n",
    "    return n * 10\n",
    "\n",
    "multiply_by_10_UDF = udf(multiply_by_10, IntegerType())\n",
    "\n",
    "df.withColumn(\"test\", multiply_by_10_UDF(\"_1\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6e72f-0496-4fbc-b7a0-18132e246059",
   "metadata": {},
   "source": [
    "# Cache and Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad73f404-a4e8-4b53-b642-3088ce680fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Deserialized 1x Replicated\n",
      "Memory Serialized 1x Replicated\n",
      "23/12/22 22:11:11 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# cache in MEMORY_AND_DISK_DESER (default)\n",
    "df.cache().count()\n",
    "assert str(df.storageLevel).startswith(\"Disk\") == True\n",
    "print(df.storageLevel)\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "# cache in MEMORY_ONLY\n",
    "df.persist(StorageLevel.MEMORY_ONLY)\n",
    "assert str(df.storageLevel).startswith(\"Memory\") == True\n",
    "print(df.storageLevel)\n",
    "\n",
    "# stores dataframe on two different executors, utilizing the executors' memory as much as possible, but not writing anything to disk.\n",
    "df.persist(StorageLevel.MEMORY_ONLY_2).count()\n",
    "\n",
    "df.unpersist()\n",
    "\n",
    "assert df.is_cached == False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cdca1-7ea9-47d6-ac25-5aef744d594f",
   "metadata": {},
   "source": [
    "# Write dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "bf17c762-dddf-4da1-908a-69c32d809e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .write\n",
    "    .partitionBy(\"_1\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(\"/home/krebai/tmp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5af6b796-f9e5-4a7a-9f0e-bf59292994e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  1|  a|\n",
      "|  2|  b|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.mode(\"overwrite\").json(\"/home/krebai/tmp/json\")\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"_1\", LongType(), True),\n",
    "    StructField(\"_2\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.read.json(\"/home/krebai/tmp/json\", schema=schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7901a-174b-48a3-95b4-2d9ccc485912",
   "metadata": {},
   "source": [
    "# Compute number of business days between 2 dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b0ce11-fc14-450d-a68e-88b4180d5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_facts = spark.createDataFrame(\n",
    "    [('data1', '2023-12-18', '2023-12-24'),\n",
    "     ('data1', '2022-05-08', '2022-05-21')],\n",
    "    ['data', 'start_date', 'end_date']\n",
    ")\n",
    "df_holidays = spark.createDataFrame([('2022-05-10',)], ['holiday_date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f938056b-602d-468d-a589-1f0c05283ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+-------------+\n",
      "| data|start_date|  end_date|business_days|\n",
      "+-----+----------+----------+-------------+\n",
      "|data1|2023-12-18|2023-12-24|            5|\n",
      "|data1|2022-05-08|2022-05-21|           10|\n",
      "+-----+----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    df_facts.withColumn(\"test\", sequence(to_date(\"start_date\"), to_date(\"end_date\")))\n",
    "    .withColumn(\"business_days\", expr(\"size(filter(test, x -> dayOfWeek(x) != 1 and dayOfWeek(x) != 7))\"))\n",
    "    .drop(\"test\")\n",
    "    .show()\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
